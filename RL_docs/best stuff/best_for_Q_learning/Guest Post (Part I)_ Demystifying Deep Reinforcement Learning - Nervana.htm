<!DOCTYPE html>
<!-- saved from url=(0068)https://www.nervanasys.com/demystifying-deep-reinforcement-learning/ -->
<html class="js no-mobile desktop no-ie chrome chrome54 demystifying-deep-reinforcement-learning-section w-1148 gt-240 gt-320 gt-480 gt-640 gt-768 gt-800 gt-1024 lt-1280 lt-1440 lt-1680 lt-1920 no-portrait landscape gradient rgba opacity textshadow multiplebgs boxshadow borderimage borderradius cssreflections csstransforms csstransitions no-touch retina fontface domloaded ua-chrome ua-chrome-54 ua-chrome-54-0 ua-chrome-54-0-2840 ua-chrome-54-0-2840-71 ua-desktop ua-desktop-macintosh ua-mac_os_x ua-mac_os_x-10 ua-mac_os_x-10-12 ua-mac_os_x-10-12-1 ua-webkit ua-webkit-537 ua-webkit-537-36 js applicationcache geolocation history postmessage websockets localstorage sessionstorage websqldatabase webworkers hashchange audio canvas canvastext video webgl cssgradients multiplebgs opacity rgba inlinesvg hsla supports svgclippaths smil no-touchevents fontface generatedcontent textshadow indexeddb indexeddb-deletedatabase cssanimations backgroundsize borderimage borderradius boxshadow flexbox cssreflections csstransforms csstransforms3d csstransitions" lang="en-US" prefix="og: http://ogp.me/ns#" id="index-page" data-useragent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">

	

	
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Guest Post (Part I): Demystifying Deep Reinforcement Learning - Nervana</title>

<!-- All in One SEO Pack 2.3.10.2 by Michael Torbert of Semper Fi Web Design[287,365] -->
<link rel="canonical" href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">
<!-- /all in one seo pack -->

<!-- This site is optimized with the Yoast SEO plugin v3.7.1 - https://yoast.com/wordpress/plugins/seo/ -->
<link rel="canonical" href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Guest Post (Part I): Demystifying Deep Reinforcement Learning - Nervana">
<meta property="og:url" content="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">
<meta property="og:site_name" content="Nervana">
<meta property="article:section" content="Developer">
<meta property="article:published_time" content="2015-12-21T18:00:20+00:00">
<meta property="article:modified_time" content="2016-10-24T17:50:32+00:00">
<meta property="og:updated_time" content="2016-10-24T17:50:32+00:00">
<meta property="og:image" content="https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-12.01.15-PM.png">
<meta property="og:image:width" content="413">
<meta property="og:image:height" content="262">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Guest Post (Part I): Demystifying Deep Reinforcement Learning - Nervana">
<meta name="twitter:image" content="https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-12.01.15-PM.png">
<!-- / Yoast SEO plugin. -->

<link rel="dns-prefetch" href="https://fonts.googleapis.com/">
<link rel="dns-prefetch" href="https://s.w.org/">
<link rel="alternate" type="application/rss+xml" title="Nervana » Feed" href="https://www.nervanasys.com/feed/">
<link rel="alternate" type="application/rss+xml" title="Nervana » Comments Feed" href="https://www.nervanasys.com/comments/feed/">
<link rel="alternate" type="text/calendar" title="Nervana » iCal Feed" href="https://www.nervanasys.com/events/?ical=1">
			<link rel="shortcut icon" href="https://www.nervanasys.com/wp-content/uploads/2016/05/favicon.png" type="image/x-icon">
					<!-- For iPhone -->
			<link rel="apple-touch-icon-precomposed" href="https://www.nervanasys.com/wp-content/uploads/2016/05/APPLE-IPHONE-ICON.png">
					<!-- For iPhone 4 Retina display -->
			<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://www.nervanasys.com/wp-content/uploads/2016/05/APPLE-IPHONE-RETINA.png">
					<!-- For iPad -->
			<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://www.nervanasys.com/wp-content/uploads/2016/05/APPLE-IPAD-ICON.png">
					<!-- For iPad Retina display -->
			<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://www.nervanasys.com/wp-content/uploads/2016/05/APPLE-IPAD-RETINA.png">
				<script async="" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/analytics.js"></script><script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/www.nervanasys.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.6.1"}};
			!function(a,b,c){function d(a){var c,d,e,f,g,h=b.createElement("canvas"),i=h.getContext&&h.getContext("2d"),j=String.fromCharCode;if(!i||!i.fillText)return!1;switch(i.textBaseline="top",i.font="600 32px Arial",a){case"flag":return i.fillText(j(55356,56806,55356,56826),0,0),!(h.toDataURL().length<3e3)&&(i.clearRect(0,0,h.width,h.height),i.fillText(j(55356,57331,65039,8205,55356,57096),0,0),c=h.toDataURL(),i.clearRect(0,0,h.width,h.height),i.fillText(j(55356,57331,55356,57096),0,0),d=h.toDataURL(),c!==d);case"diversity":return i.fillText(j(55356,57221),0,0),e=i.getImageData(16,16,1,1).data,f=e[0]+","+e[1]+","+e[2]+","+e[3],i.fillText(j(55356,57221,55356,57343),0,0),e=i.getImageData(16,16,1,1).data,g=e[0]+","+e[1]+","+e[2]+","+e[3],f!==g;case"simple":return i.fillText(j(55357,56835),0,0),0!==i.getImageData(16,16,1,1).data[0];case"unicode8":return i.fillText(j(55356,57135),0,0),0!==i.getImageData(16,16,1,1).data[0];case"unicode9":return i.fillText(j(55358,56631),0,0),0!==i.getImageData(16,16,1,1).data[0]}return!1}function e(a){var c=b.createElement("script");c.src=a,c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i;for(i=Array("simple","flag","unicode8","diversity","unicode9"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/wp-emoji-release.min.js" type="text/javascript"></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="hover_effects_pack-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/hover_pack.css" type="text/css" media="all">
<link rel="stylesheet" id="contact-form-7-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/styles.css" type="text/css" media="all">
<link rel="stylesheet" id="essential-grid-plugin-settings-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/settings.css" type="text/css" media="all">
<link rel="stylesheet" id="tp-open-sans-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/css" type="text/css" media="all">
<link rel="stylesheet" id="tp-raleway-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/css(1)" type="text/css" media="all">
<link rel="stylesheet" id="tp-droid-serif-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/css(2)" type="text/css" media="all">
<link rel="stylesheet" id="optinforms-googleFont-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/css(3)" type="text/css" media="all">
<link rel="stylesheet" id="optinforms-stylesheet-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/optinforms.css" type="text/css" media="all">
<link rel="stylesheet" id="rs-plugin-settings-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/settings(1).css" type="text/css" media="all">
<style id="rs-plugin-settings-inline-css" type="text/css">
.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_huge_white_text{position:absolute; color:#ffffff; font-size:130px; line-height:45px; font-family:museoslab500regular;   text-shadow:0px 2px 5px rgba(0,0,0,1)}.avada_huge_black_text{position:absolute; color:#000000; font-size:130px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text{position:absolute; color:#333333; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_white_text{position:absolute; color:#fff; font-size:42px; line-height:45px; font-family:museoslab500regular}.avada_big_black_text_center{position:absolute; color:#333333; font-size:38px; line-height:45px; font-family:museoslab500regular;   text-align:center}.avada_med_green_text{position:absolute; color:#A0CE4E; font-size:24px; line-height:24px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_gray_text{position:absolute; color:#747474; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_small_white_text{position:absolute; color:#fff; font-size:13px; line-height:20px; font-family:PTSansRegular,Arial,Helvetica,sans-serif;  text-shadow:0px 2px 5px rgba(0,0,0,0.5); font-weight:700}.avada_block_black{position:absolute; color:#A0CE4E; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_green{position:absolute; color:#000; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#A0CE4E;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:#000;font-family:PTSansRegular,Arial,Helvetica,sans-serif}.avada_block_white_trans{position:absolute; color:#fff; text-shadow:none; font-size:22px; line-height:34px; padding:0px 10px; padding-top:1px;margin:0px; border-width:0px; border-style:none; background-color:rgba(0,0,0,0.6);  font-family:PTSansRegular,Arial,Helvetica,sans-serif}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}.tp-caption a{color:#ff7302;text-shadow:none;-webkit-transition:all 0.2s ease-out;-moz-transition:all 0.2s ease-out;-o-transition:all 0.2s ease-out;-ms-transition:all 0.2s ease-out}.tp-caption a:hover{color:#ffa902}
</style>
<link rel="stylesheet" id="multi-color-sub-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/multicolor-subscribe-widget.css" type="text/css" media="all">
<link rel="stylesheet" id="avada-stylesheet-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/style.min.css" type="text/css" media="all">
<!--[if lte IE 9]>
<link rel='stylesheet' id='avada-shortcodes-css'  href='https://www.nervanasys.com/wp-content/themes/Avada/shortcodes.css?ver=5.0.2' type='text/css' media='all' />
<![endif]-->
<link rel="stylesheet" id="fontawesome-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/font-awesome.css" type="text/css" media="all">
<!--[if lte IE 9]>
<link rel='stylesheet' id='avada-IE-fontawesome-css'  href='https://www.nervanasys.com/wp-content/themes/Avada/assets/fonts/fontawesome/font-awesome.css?ver=5.0.2' type='text/css' media='all' />
<![endif]-->
<!--[if lte IE 8]>
<link rel='stylesheet' id='avada-IE8-css'  href='https://www.nervanasys.com/wp-content/themes/Avada/assets/css/ie8.css?ver=5.0.2' type='text/css' media='all' />
<![endif]-->
<!--[if IE]>
<link rel='stylesheet' id='avada-IE-css'  href='https://www.nervanasys.com/wp-content/themes/Avada/assets/css/ie.css?ver=5.0.2' type='text/css' media='all' />
<![endif]-->
<link rel="stylesheet" id="avada-iLightbox-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/ilightbox.css" type="text/css" media="all">
<link rel="stylesheet" id="avada-animations-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/animations.css" type="text/css" media="all">
<link rel="stylesheet" id="fusion-builder-shortcodes-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/fusion-shortcodes.min.css" type="text/css" media="all">
<link rel="stylesheet" id="avada-dynamic-css-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/avada-13232.css" type="text/css" media="all">
<link rel="stylesheet" id="avada_google_fonts-css" href="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/css(4)" type="text/css" media="all">
<script type="text/javascript">
/* <![CDATA[ */
var tribe_events_linked_posts = {"post_types":{"tribe_venue":"venue","tribe_organizer":"organizer"}};
/* ]]> */
</script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/jquery.js"></script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/jquery-migrate.min.js"></script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/hover_pack.js"></script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/lightbox.js"></script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/jquery.themepunch.tools.min.js"></script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/jquery.themepunch.essential.min.js"></script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/placeholder.js"></script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/jquery.themepunch.revolution.min.js"></script>
<!--[if lt IE 9]>
<script type='text/javascript' src='https://www.nervanasys.com/wp-content/themes/Avada/assets/js/html5shiv.js?ver=5.0.2'></script>
<![endif]-->
<!--[if lt IE 9]>
<script type='text/javascript' src='https://www.nervanasys.com/wp-content/themes/Avada/assets/js/excanvas.js?ver=5.0.2'></script>
<![endif]-->
<link rel="https://api.w.org/" href="https://www.nervanasys.com/wp-json/">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://www.nervanasys.com/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://www.nervanasys.com/wp-includes/wlwmanifest.xml"> 
<meta name="generator" content="WordPress 4.6.1">
<link rel="shortlink" href="https://www.nervanasys.com/?p=13232">
<link rel="alternate" type="application/json+oembed" href="https://www.nervanasys.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.nervanasys.com%2Fdemystifying-deep-reinforcement-learning%2F">
<link rel="alternate" type="text/xml+oembed" href="https://www.nervanasys.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.nervanasys.com%2Fdemystifying-deep-reinforcement-learning%2F&amp;format=xml">
		<script type="text/javascript">
			var ajaxRevslider;
			
			jQuery(document).ready(function() {
				// CUSTOM AJAX CONTENT LOADING FUNCTION
				ajaxRevslider = function(obj) {
				
					// obj.type : Post Type
					// obj.id : ID of Content to Load
					// obj.aspectratio : The Aspect Ratio of the Container / Media
					// obj.selector : The Container Selector where the Content of Ajax will be injected. It is done via the Essential Grid on Return of Content
					
					var content = "";

					data = {};
					
					data.action = 'revslider_ajax_call_front';
					data.client_action = 'get_slider_html';
					data.token = '75648d7b5f';
					data.type = obj.type;
					data.id = obj.id;
					data.aspectratio = obj.aspectratio;
					
					// SYNC AJAX REQUEST
					jQuery.ajax({
						type:"post",
						url:"https://www.nervanasys.com/wp-admin/admin-ajax.php",
						dataType: 'json',
						data:data,
						async:false,
						success: function(ret, textStatus, XMLHttpRequest) {
							if(ret.success == true)
								content = ret.data;								
						},
						error: function(e) {
							console.log(e);
						}
					});
					
					 // FIRST RETURN THE CONTENT WHEN IT IS LOADED !!
					 return content;						 
				};
				
				// CUSTOM AJAX FUNCTION TO REMOVE THE SLIDER
				var ajaxRemoveRevslider = function(obj) {
					return jQuery(obj.selector+" .rev_slider").revkill();
				};

				// EXTEND THE AJAX CONTENT LOADING TYPES WITH TYPE AND FUNCTION
				var extendessential = setInterval(function() {
					if (jQuery.fn.tpessential != undefined) {
						clearInterval(extendessential);
						if(typeof(jQuery.fn.tpessential.defaults) !== 'undefined') {
							jQuery.fn.tpessential.defaults.ajaxTypes.push({type:"revslider",func:ajaxRevslider,killfunc:ajaxRemoveRevslider,openAnimationSpeed:0.3});   
							// type:  Name of the Post to load via Ajax into the Essential Grid Ajax Container
							// func: the Function Name which is Called once the Item with the Post Type has been clicked
							// killfunc: function to kill in case the Ajax Window going to be removed (before Remove function !
							// openAnimationSpeed: how quick the Ajax Content window should be animated (default is 0.3)
						}
					}
				},30);
			});
		</script>
		<script type="text/javascript">
	window._se_plugin_version = '8.1.6';
</script>
<style type="text/css" id="custom-background-css">
body.custom-background { background-color: #eaeaea; }
</style>
<meta name="generator" content="Powered by Slider Revolution 5.2.5 - responsive, Mobile-Friendly Slider Plugin for WordPress with comfortable drag and drop interface.">
<style type="text/css" id="syntaxhighlighteranchor"></style>

	
	<script type="text/javascript">
		var doc = document.documentElement;
		doc.setAttribute('data-useragent', navigator.userAgent);
	</script>

	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-49905241-3', 'auto');
  ga('send', 'pageview');

</script>
	<style type="text/css">.esgbox-margin{margin-right:0px;}</style><style id="fit-vids-style">.fluid-width-video-wrapper{width:100%;position:relative;padding:0;}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper object,.fluid-width-video-wrapper embed {position:absolute;top:0;left:0;width:100%;height:100%;}</style></head>
<body class="single single-post postid-13232 single-format-standard custom-background fusion-body no-totop mobile-logo-pos-left layout-wide-mode has-sidebar menu-text-align-left mobile-menu-design-modern fusion-image-hovers fusion-show-pagination-text do-animate">
				<div id="wrapper" class="">
		<div id="home" style="position:relative;top:1px;"></div>
				
		
			<header class="fusion-header-wrapper fusion-is-sticky">
				<div class="fusion-header-v1 fusion-logo-left fusion-sticky-menu- fusion-sticky-logo-1 fusion-mobile-logo- fusion-mobile-menu-design-modern ">
					<div class="fusion-header-sticky-height" style="display: block; height: 65.1073px; overflow: hidden;"></div>
<div class="fusion-header fusion-sticky-shadow" style="top: 0px; height: 65.1073px; overflow: hidden;">
	<div class="fusion-row" style="padding-top: 0px; padding-bottom: 0px;">
		
<div class="fusion-logo" data-margin-top="31px" data-margin-bottom="31px" data-margin-left="0px" data-margin-right="0px" style="margin-top: 14.0536px; margin-bottom: 14.0536px;">
				<a class="fusion-logo-link" href="https://www.nervanasys.com/">
						<img src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/logo200.png" width="" height="" alt="Nervana" class="fusion-logo-1x fusion-standard-logo">

							<img src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/logo200.png" width="" height="" alt="Nervana" class="fusion-standard-logo fusion-logo-2x">
			
			<!-- mobile logo -->
			
			<!-- sticky header logo -->
											<img src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/logo200-1.png" width="" height="" alt="Nervana" class="fusion-logo-1x fusion-sticky-logo-1x">

									<img src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/logo200-1.png" width="" height="" alt="Nervana" class="fusion-logo-2x fusion-sticky-logo-2x" data-logo-height="37" data-logo-width="200" style="overflow: hidden; height: 37px;">
									</a>
		</div>		<nav class="fusion-main-menu"><ul id="menu-main-menu" class="fusion-menu"><li id="menu-item-11021" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11021"><a title="Nervana Systems" href="https://www.nervanasys.com/" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text">HOME</span></a></li><li id="menu-item-11235" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11235"><a href="https://www.nervanasys.com/products/" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text">PRODUCTS</span></a></li><li id="menu-item-14604" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-14604 fusion-dropdown-menu"><a href="https://www.nervanasys.com/technology/" aria-haspopup="true" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text">TECHNOLOGY</span></a><ul class="sub-menu"><li id="menu-item-14628" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-14628 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/technology/neon/"><span class="">Neon</span></a></li><li id="menu-item-15360" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-15360 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/technology/engine/"><span class="">Nervana Engine</span></a></li></ul></li><li id="menu-item-13929" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-13929 fusion-dropdown-menu"><a href="https://www.nervanasys.com/solutions/" aria-haspopup="true" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text">SOLUTIONS</span></a><ul class="sub-menu"><li id="menu-item-13930" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13930 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/solutions/healthcare/"><span class="">Healthcare</span></a></li><li id="menu-item-13931" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13931 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/solutions/agriculture/"><span class="">Agriculture</span></a></li><li id="menu-item-13932" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13932 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/solutions/finance/"><span class="">Finance</span></a></li><li id="menu-item-13933" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13933 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/solutions/onlineservices/"><span class="">Online Services</span></a></li><li id="menu-item-13934" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13934 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/solutions/automotive/"><span class="">Automotive</span></a></li><li id="menu-item-13935" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13935 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/solutions/energy/"><span class="">Energy</span></a></li></ul></li><li id="menu-item-11022" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-11022 fusion-dropdown-menu"><a href="https://www.nervanasys.com/about/" aria-haspopup="true" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text">COMPANY</span></a><ul class="sub-menu"><li id="menu-item-13927" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13927 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/team/"><span class="">Team</span></a></li><li id="menu-item-13924" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13924 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/careers/"><span class="">Careers</span></a></li><li id="menu-item-15654" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-15654 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/culture/"><span class="">Culture</span></a></li><li id="menu-item-13928" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13928 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/investors-and-advisors/"><span class="">Investors &amp; Advisors</span></a></li></ul></li><li id="menu-item-13926" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-13926 fusion-dropdown-menu"><a href="https://www.nervanasys.com/press/" aria-haspopup="true" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text">PRESS</span></a><ul class="sub-menu"><li id="menu-item-15592" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-15592 fusion-dropdown-submenu"><a href="https://www.nervanasys.com/press/intel-nervana-news/"><span class="">Intel + Nervana News</span></a></li></ul></li><li id="menu-item-13925" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13925"><a href="https://www.nervanasys.com/blog/" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text">BLOG</span></a></li><li id="menu-item-15913" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-15913"><a href="https://www.nervanasys.com/learn/" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text">LEARN</span></a></li><li id="menu-item-11028" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11028 fusion-menu-item-button fusion-last-menu-item"><a href="https://www.nervanasys.com/contact/" style="line-height: 62.0663px; height: 62.0663px; overflow: hidden;"><span class="menu-text fusion-button button-default button-medium">GET STARTED</span></a></li></ul></nav>			<div class="fusion-mobile-menu-icons">
							<a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/#" class="fusion-icon fusion-icon-bars"></a>
		
		
			</div>


<nav class="fusion-mobile-nav-holder" style="display: none;"><ul id="menu-main-menu" class="fusion-menu"><li id="mobile-menu-item-11021" class="fusion-mobile-nav-item" style=""><a title="Nervana Systems" href="https://www.nervanasys.com/"><span class="menu-text">HOME</span></a></li><li id="mobile-menu-item-11235" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/products/"><span class="menu-text">PRODUCTS</span></a></li><li id="mobile-menu-item-14604" class="fusion-mobile-nav-item" style=""><span href="#" aria-haspopup="true" class="fusion-open-submenu"></span><a href="https://www.nervanasys.com/technology/"><span class="menu-text">TECHNOLOGY</span></a><ul class="sub-menu" style="display: none;"><li id="mobile-menu-item-14628" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/technology/neon/"><span class="">Neon</span></a></li><li id="mobile-menu-item-15360" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/technology/engine/"><span class="">Nervana Engine</span></a></li></ul></li><li id="mobile-menu-item-13929" class="fusion-mobile-nav-item" style=""><span href="#" aria-haspopup="true" class="fusion-open-submenu"></span><a href="https://www.nervanasys.com/solutions/"><span class="menu-text">SOLUTIONS</span></a><ul class="sub-menu" style="display: none;"><li id="mobile-menu-item-13930" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/solutions/healthcare/"><span class="">Healthcare</span></a></li><li id="mobile-menu-item-13931" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/solutions/agriculture/"><span class="">Agriculture</span></a></li><li id="mobile-menu-item-13932" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/solutions/finance/"><span class="">Finance</span></a></li><li id="mobile-menu-item-13933" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/solutions/onlineservices/"><span class="">Online Services</span></a></li><li id="mobile-menu-item-13934" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/solutions/automotive/"><span class="">Automotive</span></a></li><li id="mobile-menu-item-13935" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/solutions/energy/"><span class="">Energy</span></a></li></ul></li><li id="mobile-menu-item-11022" class="fusion-mobile-nav-item" style=""><span href="#" aria-haspopup="true" class="fusion-open-submenu"></span><a href="https://www.nervanasys.com/about/"><span class="menu-text">COMPANY</span></a><ul class="sub-menu" style="display: none;"><li id="mobile-menu-item-13927" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/team/"><span class="">Team</span></a></li><li id="mobile-menu-item-13924" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/careers/"><span class="">Careers</span></a></li><li id="mobile-menu-item-15654" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/culture/"><span class="">Culture</span></a></li><li id="mobile-menu-item-13928" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/investors-and-advisors/"><span class="">Investors &amp; Advisors</span></a></li></ul></li><li id="mobile-menu-item-13926" class="fusion-mobile-nav-item" style=""><span href="#" aria-haspopup="true" class="fusion-open-submenu"></span><a href="https://www.nervanasys.com/press/"><span class="menu-text">PRESS</span></a><ul class="sub-menu" style="display: none;"><li id="mobile-menu-item-15592" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/press/intel-nervana-news/"><span class="">Intel + Nervana News</span></a></li></ul></li><li id="mobile-menu-item-13925" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/blog/"><span class="menu-text">BLOG</span></a></li><li id="mobile-menu-item-15913" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/learn/"><span class="menu-text">LEARN</span></a></li><li id="mobile-menu-item-11028" class="fusion-mobile-nav-item" style=""><a href="https://www.nervanasys.com/contact/"><span class="menu-text">GET STARTED</span></a></li></ul></nav>

	</div>
</div>
				</div>
				<div class="fusion-clearfix"></div>
			</header>
					
		<div id="sliders-container">
					</div>
				
							
		
		
						<div id="main" class="clearfix " style="">
			<div class="fusion-row" style="">

<div id="content" style="float: left;">

	
			<article id="post-13232" class="post post-13232 type-post status-publish format-standard has-post-thumbnail hentry category-developer">
						
																			
							<h2 class="entry-title fusion-post-title" data-fontsize="42" data-lineheight="45">Guest Post (Part I): Demystifying Deep Reinforcement Learning</h2>						<div class="post-content">
				<div class="fusion-fullwidth fullwidth-box hundred-percent-fullwidth" style="background-color: rgba(255,255,255,0);background-position: center center;background-repeat: no-repeat;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;"><div class="fusion-builder-row fusion-row "><div class="fusion-layout-column fusion_builder_column fusion_builder_column_1_1  fusion-one-full fusion-column-first fusion-column-last fusion-column-no-min-height 1_1" style="margin-top:0px;margin-bottom:0px;">
			<div class="fusion-column-wrapper" style="background-position:left top;background-repeat:no-repeat;-webkit-background-size:cover;-moz-background-size:cover;-o-background-size:cover;background-size:cover;" data-bg-url="">
				<p>Two years ago, a small company in London called DeepMind uploaded their pioneering paper “<a href="http://arxiv.org/abs/1312.5602" target="_blank">Playing Atari with Deep Reinforcement Learning</a>” to Arxiv. In this paper they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased.&nbsp;The result was remarkable, because the games and the goals in every game were very different and designed to be challenging for humans. The same model architecture, without any change, was used to learn seven different games, and in three of them the algorithm performed even better than a human!</p>
<p>It has been hailed since then as the first step towards <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank">general artificial intelligence</a> – an AI that can survive in a variety of environments, instead of being confined to strict realms such as playing chess. No wonder <a href="http://techcrunch.com/2014/01/26/google-deepmind/" target="_blank">DeepMind was immediately bought by Google</a> and has been on the forefront of deep learning research ever since. In February 2015 their paper “<a href="http://www.nature.com/articles/nature14236" target="_blank">Human-level control through deep reinforcement learning</a>” was featured on the cover of Nature, one of the most prestigious journals in science. In this paper they applied the same model to 49 different games and achieved superhuman performance in half of them.</p>
<p>Still, while deep models for supervised and unsupervised learning have seen widespread adoption in the community, deep reinforcement learning has remained a bit of a mystery. In this blog post I will be trying to demystify this technique and understand the rationale behind it. The intended audience is someone who already has background in machine learning and possibly in neural networks, but hasn’t had time to delve into reinforcement learning yet.</p>
<p>The roadmap ahead:</p>
<ol>
<li><strong>What are the main challenges in reinforcement learning?</strong> We will cover the credit assignment problem and the exploration-exploitation dilemma here.</li>
<li><strong>How to formalize reinforcement learning in mathematical terms?</strong> We will define Markov Decision Process and use it for reasoning about reinforcement learning.</li>
<li><strong>How do we form long-term strategies?</strong> We define “discounted future reward”, that forms the main basis for the algorithms in the next sections.</li>
<li><strong>How can we estimate or approximate the future reward?</strong> Simple table-based Q-learning algorithm is defined and explained here.</li>
<li><strong>What if our state space is too big?</strong> Here we see how Q-table can be replaced with a (deep) neural network.</li>
<li><strong>What do we need to make it actually work?</strong> Experience replay technique will be discussed here, that stabilizes the learning with neural networks.</li>
<li><strong>Are we done yet?</strong> Finally we will consider some simple solutions to the exploration-exploitation problem.
</li>
</ol>
<h1 data-fontsize="34" data-lineheight="49">Reinforcement Learning</h1>
<p>Consider the game Breakout. In this game you control a paddle at the bottom of the screen and have to bounce the ball back to clear all the bricks in the upper half of the screen. Each time you hit a brick, it disappears and your score increases – you get a reward.</p>
<div class="imageframe-align-center"><span class="fusion-imageframe imageframe-none imageframe-1 hover-type-none fusion-animated" data-animationtype="fadeInDown" data-animationduration="1" data-animationoffset="100%" style="visibility: visible; animation-duration: 1s;"><img src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.08.53-AM.png" width="" height="" alt="" title="" class="img-responsive"></span></div><p><sup>Figure 1: Atari Breakout game. Image credit: DeepMind.</sup></p>
<p>Suppose you want to teach a neural network to play this game. Input to your network would be screen images, and output would be three actions: left, right or fire (to launch the ball). It would make sense to treat it as a classification problem – for each game screen you have to decide, whether you should move left, right or press fire. Sounds straightforward? Sure, but then you need training examples, and a lots of them. Of course you could go and record game sessions using expert players, but that’s not really how we learn. We don’t need somebody to tell us a million times which move to choose at each screen. We just need occasional feedback that we did the right thing and can then figure out everything else ourselves.</p>
<p>This is the task <strong>reinforcement learning </strong>tries to solve. Reinforcement learning lies somewhere in between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has <u>sparse</u> and <u>time-delayed</u> labels – the rewards. Based only on those rewards the agent has to learn to behave in the environment.</p>
<p>While the idea is quite intuitive, in practice there are numerous challenges. For example when you hit a brick and score a reward in the Breakout game, it often has nothing to do with the actions (paddle movements) you did <u>just before</u> getting the reward. All the hard work was already done, when you positioned the paddle correctly and bounced the ball back. This is called the <strong>credit assignment problem</strong> – i.e., which of the preceding actions was responsible for getting the reward and to what extent.</p>
<p>Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards? In the above Breakout game a simple strategy is to move to the left edge and wait there. When launched, the ball tends to fly left more often than right and you will easily score about 10 points before you die. Will you be satisfied with this or do you want more? This is called the <strong>explore-exploit dilemma</strong> – should you exploit the known working strategy or explore other, possibly better strategies.</p>
<p>Reinforcement learning is an important model of how we (and all animals in general) learn. Praise from our parents, grades in school, salary at work – these are all examples of rewards. Credit assignment problems and exploration-exploitation dilemmas come up every day both in business and in relationships. That’s why it is important to study this problem, and games form a wonderful sandbox for trying out new approaches.</p>
<h1 data-fontsize="34" data-lineheight="49">Markov Decision Process</h1>
<p>Now the question is, how do you formalize a reinforcement learning problem, so that you can reason about it? The most common method is to represent it as a Markov decision process.</p>
<p>Suppose you are an <strong>agent</strong>, situated in an <strong>environment</strong> (e.g. Breakout game). The environment is in a certain <strong>state</strong> (e.g. location of the paddle, location and direction of the ball, existence of every brick and so on). The agent can perform certain <strong>actions</strong> in the environment (e.g. move the paddle to the left or to the right). These actions sometimes result in a <strong>reward</strong> (e.g. increase in score). Actions transform the environment and lead to a new state, where the agent can perform another action, and so on. The rules for how you choose those actions are called <strong>policy</strong>. The environment in general is stochastic, which means the next state may be somewhat random (e.g. when you lose a ball and launch a new one, it goes towards a random direction).</p>
<div class="imageframe-align-center"><span class="fusion-imageframe imageframe-none imageframe-2 hover-type-none fusion-animated" data-animationtype="fadeInDown" data-animationduration="1" data-animationoffset="100%" style="visibility: visible; animation-duration: 1s;"><img src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-12.01.04-PM.png" width="" height="" alt="" title="" class="img-responsive"></span></div><p><sup>Figure 2:</sup> <sup><em>Left: </em>reinforcement learning problem. <em>Right: </em>Markov decision process.</sup></p>
<p></p>
<p>The set of states and actions, together with rules for transitioning from one state to another, make up a <strong>Markov decision process</strong>. One <strong>episode</strong> of this process (e.g. one game) forms a finite sequence of states, actions and rewards:</p>
<p style="text-align: center;"><img class="alignnone size-full wp-image-13235" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.09.19-AM.png" alt="Screen Shot 2015-12-21 at 11.09.19 AM" width="373" height="40" srcset="https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.19-AM-300x32.png 300w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.19-AM.png 373w" sizes="(max-width: 373px) 100vw, 373px"></p>
<p>Here s<sub>i</sub> represents the state, a<sub>i</sub> is the action and r<sub>i+1</sub> is the reward after performing the action. The episode ends with <strong>terminal</strong> state s<sub>n</sub> (e.g. “game over” screen). A Markov decision process relies on the Markov assumption, that the probability of the next state s<sub>i+1</sub> depends only on current state s<sub>i</sub> and action a<sub>i</sub>, but not on preceding states or actions.</p>
<h1 data-fontsize="34" data-lineheight="49">Discounted Future Reward</h1>
<p>To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. How should we go about that?</p>
<p>Given one run of the Markov decision process, we can easily calculate the <strong>total reward</strong> for one episode:</p>
<p><img class="size-full wp-image-13236 aligncenter" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.09.26-AM.png" alt="Screen Shot 2015-12-21 at 11.09.26 AM" width="231" height="42"></p>
<p>Given that, the <strong>total future reward</strong> from time point <em>t</em> onward can be expressed as:</p>
<p><img class="size-full wp-image-13237 aligncenter" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.09.32-AM.png" alt="Screen Shot 2015-12-21 at 11.09.32 AM" width="275" height="38"></p>
<p>But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use <strong>discounted future reward </strong>instead:</p>
<p><img class="size-full wp-image-13238 aligncenter" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.09.36-AM.png" alt="Screen Shot 2015-12-21 at 11.09.36 AM" width="326" height="33" srcset="https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.36-AM-300x30.png 300w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.36-AM-320x33.png 320w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.36-AM.png 326w" sizes="(max-width: 326px) 100vw, 326px"></p>
<p>Here <em>γ</em> is the discount factor between 0 and 1 – the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step <em>t</em> can be expressed in terms of the same thing at time step <em>t+1</em>:</p>
<p><img class="size-full wp-image-13239 aligncenter" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.09.40-AM.png" alt="Screen Shot 2015-12-21 at 11.09.40 AM" width="393" height="37" srcset="https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.40-AM-300x28.png 300w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.40-AM.png 393w" sizes="(max-width: 393px) 100vw, 393px"></p>
<p>If we set the discount factor <em>γ</em>=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like <em>γ=</em>0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor <em>γ</em>=1.</p>
<p>A good strategy for an agent would be to <strong>always choose an action that maximizes the (discounted) future reward</strong>.</p>
<h1 data-fontsize="34" data-lineheight="49">Q-learning</h1>
<p>In Q-learning we define a function <em>Q(s, a)</em> representing <strong>the maximum discounted future reward when we perform action </strong><u>a</u><strong> in state </strong><u>s</u><strong>, and continue optimally from that point on.</strong></p><strong>
<p><img class="size-full wp-image-13240 aligncenter" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.09.47-AM.png" alt="Screen Shot 2015-12-21 at 11.09.47 AM" width="204" height="32"></p>
</strong><p><strong></strong>The way to think about <em>Q(s, a)</em>&nbsp;is that it is “the best possible score at the end of the game after performing action <u>a</u><strong> in state </strong><u>s</u><strong>“. It is called Q-function, because it represents the “quality” of a certain action in a given state.</strong></p>
<p>This may sound like quite a puzzling definition. How can we estimate the score at the end of game, if we know just the current state and action, and not the actions and rewards coming after that? We really can’t. But as a theoretical construct we can assume existence of such a function. Just close your eyes and repeat to yourself five times: “<em>Q(s, a)&nbsp;</em>exists, <em>Q(s, a)&nbsp;</em>exists, …”. Feel it?</p>
<p>If you’re still not convinced, then consider what the implications of having such a function would be. Suppose you are in state and pondering whether you should take action <em>a</em> or <em>b</em>. You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer becomes really simple – pick the action with the highest Q-value!</p>
<p><img class="size-full wp-image-13242 aligncenter" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.09.56-AM.png" alt="Screen Shot 2015-12-21 at 11.09.56 AM" width="246" height="36"></p>
<p>Here π represents the policy, the rule how we choose an action in each state.</p>
<p>OK, how do we get that Q-function then? Let’s focus on just one transition &lt;<em>s, a, r, s’</em>&gt;. Just like with discounted future rewards in the previous section, we can express the Q-value of state <em>s</em> and action <em>a</em> in terms of the Q-value of the next state <em>s’</em>.</p>
<p><img class="size-full wp-image-13241 aligncenter" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.10.00-AM.png" alt="Screen Shot 2015-12-21 at 11.10.00 AM" width="284" height="29"></p>
<p>This is called the <strong>Bellman equation</strong>. If you think about it, it is quite logical – maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state.</p>
<p>The main idea in Q-learning is that <strong>we can iteratively approximate the Q-function using the Bellman equation</strong>. In the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. The gist of the Q-learning algorithm is as simple as the following<a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/#_ftn1" name="_ftnref1"><sup><sup>[1]</sup></sup></a>:</p>
<p><img class="alignnone size-full wp-image-13243" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.23.55-AM.png" alt="Screen Shot 2015-12-21 at 11.23.55 AM" width="706" height="227" srcset="https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.55-AM-300x96.png 300w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.55-AM-560x180.png 560w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.55-AM.png 706w" sizes="(max-width: 706px) 100vw, 706px"></p>
<p><em><u>α</u></em>&nbsp;in the algorithm is a learning rate that controls how much of the difference between previous Q-value and newly proposed Q-value is taken into account. In particular, when <em><u>α</u></em>=1, then two <em>Q[s,a]</em> cancel and the update is exactly the same as the Bellman equation.</p>
<p>The max<em><sub>a’</sub></em> <em>Q</em>[<em>s’</em>,<em>a’</em>] that we use to update <em>Q</em>[<em>s</em>,<em>a</em>] is only an approximation and in early stages of learning it may be completely wrong. However the approximation get more and more accurate with every iteration and <a href="http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf" target="_blank">it has been shown</a>, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.</p>
<h1 data-fontsize="34" data-lineheight="49">Deep Q Network</h1>
<p>The state of the environment in the Breakout game can be defined by the location of the paddle, location and direction of the ball and the presence or absence of each individual brick. This intuitive representation however is game specific. Could we come up with something more universal, that would be suitable for all the games? The obvious choice is screen pixels – they implicitly contain all of the relevant information about the game situation, except for the speed and direction of the ball. Two consecutive screens would have these covered as well.</p>
<p>If we apply the same preprocessing to game screens as in the DeepMind paper – take the four last screen images, resize them to 84×84 and convert to grayscale with 256 gray levels – we would have 256<sup>84x84x4</sup> ≈ 10<sup>67970</sup> possible game states. This means 10<sup>67970</sup> rows in our imaginary Q-table – more than the number of atoms in the known universe! One could argue that many pixel combinations (and therefore states) never occur – we could possibly represent it as a sparse table containing only visited states. Even so, most of the states are very rarely visited and it would take a lifetime of the universe for the Q-table to converge. Ideally, we would also like to have a good guess for Q-values for states we have never seen before.</p>
<p>This is the point where deep learning steps in. Neural networks are exceptionally good at coming up with good features for highly structured data. We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. This approach has the advantage, that if we want to perform a Q-value update or pick the action with the highest Q-value, we only have to do one forward pass through the network and have all Q-values for all actions available immediately.</p>
<div class="imageframe-align-center"><span class="fusion-imageframe imageframe-none imageframe-3 hover-type-none fusion-animated" data-animationtype="fadeInDown" data-animationduration="1" data-animationoffset="100%" style="visibility: visible; animation-duration: 1s;"><img src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.27.12-AM.png" width="" height="" alt="" title="" class="img-responsive"></span></div>
<p style="text-align: center;"></p><p><sup>Figure 3:</sup> <sup><em>Left: </em>Naive formulation of deep Q-network. <em>Right: </em>More optimized architecture of deep Q-network, used in DeepMind paper.</sup></p>
<p>The network architecture that DeepMind used is as follows:</p>
<p><img class="alignnone size-full wp-image-13247" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.23.28-AM.png" alt="Screen Shot 2015-12-21 at 11.23.28 AM" width="871" height="280" srcset="https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.28-AM-300x96.png 300w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.28-AM-560x180.png 560w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.28-AM-768x247.png 768w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.28-AM.png 871w" sizes="(max-width: 871px) 100vw, 871px"></p>
<p>This is a classical convolutional neural network with three convolutional layers, followed by two fully connected layers. People familiar with object recognition networks may notice that there are no pooling layers. But if you really think about it, pooling layers buy you translation invariance – the network becomes insensitive to the location of an object in the image. That makes perfectly sense for a classification task like ImageNet, but for games the location of the ball is crucial in determining the potential reward and we wouldn’t want to discard this information!</p>
<p>Input to the network are four 84×84 grayscale game screens. Outputs of the network are Q-values for each possible action (18 in Atari). Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.</p>
<p><img class="alignnone wp-image-13288 size-full" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/formula.png" alt="" width="369" height="97" srcset="https://www.nervanasys.com/wp-content/uploads/2015/12/formula-300x79.png 300w, https://www.nervanasys.com/wp-content/uploads/2015/12/formula.png 369w" sizes="(max-width: 369px) 100vw, 369px"></p>
<p>Given a transition &lt;<em> s, a, r, s’</em> &gt;, the Q-table update rule in the previous algorithm must be replaced with the following:</p>
<ol>
<li>Do a feedforward pass for the current state <em>s</em> to get predicted Q-values for all actions.</li>
<li>Do a feedforward pass for the next state <em>s’&nbsp;</em>and calculate maximum overall network outputs <em>max&nbsp;<sub>a’&nbsp;</sub>Q(s’, a’).</em></li>
<li>Set Q-value target for action to <em>r +&nbsp;γmax&nbsp;<sub>a’&nbsp;</sub>Q(s’, a’)</em>&nbsp;(use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.</li>
<li>Update the weights using backpropagation.</li>
</ol>
<h1 data-fontsize="34" data-lineheight="49">Experience Replay</h1>
<p>By now we have an idea how to estimate the future reward in each state using Q-learning and approximate the Q-function using a convolutional neural network. But it turns out that approximation of Q-values using non-linear functions is not very stable. There is a whole bag of tricks that you have to use to actually make it converge. And it takes a long time, almost a week on a single GPU.</p>
<p>The most important trick is <strong>experience replay</strong>. During gameplay all the experiences &lt;<em>&nbsp;s, a, r, s’</em>&nbsp;&gt; are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.</p>
<h1 data-fontsize="34" data-lineheight="49">Exploration-Exploitation</h1>
<p>Q-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward. But we haven’t touched the exploration-exploitation dilemma yet…</p>
<p>Firstly observe, that when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds.</p>
<p>A simple and effective fix for the above problem is <strong><em>ε</em>-greedy exploration</strong> – with probability <em>ε</em> choose a random action, otherwise go with the “greedy” action with the highest Q-value. In their system DeepMind actually decreases <em>ε</em> over time from 1 to 0.1 – in the beginning the system makes completely random moves to explore the state space maximally, and then it settles down to a fixed exploration rate.</p>
<h1 data-fontsize="34" data-lineheight="49">Deep Q-learning Algorithm</h1>
<p>This gives us the final deep Q-learning algorithm with experience replay:</p>
<p><img class="alignnone size-full wp-image-13248" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/Screen-Shot-2015-12-21-at-11.23.43-AM-1.png" alt="Screen Shot 2015-12-21 at 11.23.43 AM" width="894" height="501" srcset="https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.43-AM-1-300x168.png 300w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.43-AM-1-560x314.png 560w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.43-AM-1-768x430.png 768w, https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.23.43-AM-1.png 894w" sizes="(max-width: 894px) 100vw, 894px"></p>
<p>There are many more tricks that DeepMind used to actually make it work – like target network, error clipping, reward clipping etc, but these are out of scope for this introduction.</p>
<p>The most amazing part of this algorithm is that it learns anything at all. Just think about it – because our Q-function is initialized randomly, it initially outputs complete garbage. And we are using this garbage (the maximum Q-value of the next state) as targets for the network, only occasionally folding in a tiny reward. That sounds insane, how could it learn anything meaningful at all? The fact is, that it does.</p>
<h1 data-fontsize="34" data-lineheight="49">Final notes</h1>
<p>Many improvements to deep Q-learning have been proposed since its first introduction – <a href="http://arxiv.org/abs/1509.06461" target="_blank">Double Q-learning</a>, <a href="http://arxiv.org/abs/1511.05952" target="_blank">Prioritized Experience Replay</a>, <a href="http://arxiv.org/abs/1511.06581" target="_blank">Dueling Network Architecture</a> and <a href="http://arxiv.org/abs/1509.02971">extension to continuous action space</a> to name a few. For latest advancements check out the <a href="http://rll.berkeley.edu/deeprlworkshop/" target="_blank">NIPS 2015 deep reinforcement learning workshop</a> and <a href="https://cmt.research.microsoft.com/ICLR2016Conference/Protected/PublicComment.aspx" target="_blank">ICLR 2016</a> (search for “reinforcement” in title). But beware, that <a href="http://www.google.com/patents/US20150100530" target="_blank">deep Q-learning has been patented by Google</a>.</p>
<p>It is often said, that artificial intelligence is something we haven’t figured out yet. Once we know how it works, it doesn’t seem intelligent any more. But deep Q-networks still continue to amaze me. Watching them figure out a new game is like observing an animal in the wild – a rewarding experience by itself.</p>
<h1 data-fontsize="34" data-lineheight="49">Credits</h1>
<p>Thanks to Ardi Tampuu, Tanel Pärnamaa, Jaan Aru, Ilya Kuzovkin, Arjun Bansal and Urs K<u>ö</u>ster for comments and suggestions on the drafts of this post.</p>
<h1 data-fontsize="34" data-lineheight="49">Links</h1>
<ul>
<li><a href="http://videolectures.net/rldm2015_silver_reinforcement_learning/" target="_blank">David Silver’s lecture about deep reinforcement learning</a></li>
<li><a href="https://www.youtube.com/watch?v=b1a53hE0yQs" target="_blank">Slightly awkward but accessible illustration of Q-learning</a></li>
<li><a href="http://rll.berkeley.edu/deeprlcourse/" target="_blank">UC Berkley’s course on deep reinforcement learning</a></li>
<li><a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank">David Silver’s reinforcement learning course</a></li>
<li><a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" target="_blank">Nando de Freitas’ course on machine learning</a> (two lectures about reinforcement learning in the end)</li>
<li><a href="http://cs231n.github.io/" target="_blank">Andrej Karpathy’s course on convolutional neural networks</a></li>
</ul>
<p><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/#_ftnref1" name="_ftn1"><sup><sup>[1]</sup></sup></a> Algorithm adapted from <a href="http://artint.info/html/ArtInt_265.html" target="_blank">http://artint.info/html/ArtInt_265.html</a></p>

<sub>This blog was first published at:&nbsp;<a href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" target="_blank" rel="noreferrer">http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/</a></sub><p></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h4 data-fontsize="18" data-lineheight="19">This is the part 1 of my series on deep reinforcement learning. Tune in next week for <a href="https://www.nervanasys.com/deep-reinforcement-learning-with-neon/" target="_blank">“Deep Reinforcement Learning with Neon”</a> for an actual implementation with <a href="https://github.com/NervanaSystems/neon" target="_blank">Neon</a> deep learning toolkit.</h4>
<div class="fusion-clearfix"></div>
			</div>
		</div></div></div>
							</div>

							<div class="fusion-meta-info"><div class="fusion-meta-info-wrapper">By <span class="vcard"><span class="fn"><a href="https://www.nervanasys.com/author/tambet/" title="Posts by Tambet Matiisen" rel="author">Tambet Matiisen</a></span></span><span class="fusion-inline-sep">|</span>
	
	
			<span class="updated" style="display:none;">
			2016-10-24T17:50:32+00:00		</span>
	
<span>Monday, December 21, 2015</span><span class="fusion-inline-sep">|</span><a href="https://www.nervanasys.com/category/developer/" rel="category tag">Developer</a><span class="fusion-inline-sep">|</span></div></div>							<div class="fusion-sharing-box fusion-single-sharing-box share-box">
				<h4 data-fontsize="18" data-lineheight="19">Share This Story, Choose Your Platform!</h4>
				<div class="fusion-social-networks"><div class="fusion-social-networks-wrapper"><a class="fusion-social-network-icon fusion-tooltip fusion-facebook fusion-icon-facebook" style="color:#84aead;" href="http://www.facebook.com/sharer.php?m2w&amp;s=100&amp;p[url]=https://www.nervanasys.com/demystifying-deep-reinforcement-learning/&amp;p[images][0]=https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-12.01.15-PM.png&amp;p[title]=Guest%20Post%20%28Part%20I%29%3A%20Demystifying%20Deep%20Reinforcement%20Learning" target="_blank" rel="noopener noreferrer" data-placement="top" data-title="Facebook" data-toggle="tooltip" title="" data-original-title="Facebook"><span class="screen-reader-text">Facebook</span></a><a class="fusion-social-network-icon fusion-tooltip fusion-twitter fusion-icon-twitter" style="color:#84aead;" href="https://twitter.com/share?text=Guest%20Post%20%28Part%20I%29%3A%20Demystifying%20Deep%20Reinforcement%20Learning&amp;url=https%3A%2F%2Fwww.nervanasys.com%2Fdemystifying-deep-reinforcement-learning%2F" target="_blank" rel="noopener noreferrer" data-placement="top" data-title="Twitter" data-toggle="tooltip" title="" data-original-title="Twitter"><span class="screen-reader-text">Twitter</span></a><a class="fusion-social-network-icon fusion-tooltip fusion-linkedin fusion-icon-linkedin" style="color:#84aead;" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.nervanasys.com/demystifying-deep-reinforcement-learning/&amp;title=Guest%20Post%20%28Part%20I%29%3A%20Demystifying%20Deep%20Reinforcement%20Learning&amp;summary=Two%20years%20ago%2C%20a%20small%20company%20in%20London%20called%20DeepMind%20uploaded%20their%20pioneering%20paper%20%E2%80%9CPlaying%20Atari%20with%20Deep%20Reinforcement%20Learning%E2%80%9D%20to%20Arxiv.%20In%20this%20paper%20they%20demonstrated%20how%20a%20computer%20learned%20to%20play%20Atari%202600%20video%20games%20by%20observing%20just%20the%20screen%20pixels%20and%20receiving%20a%20reward%20when%20the%20game%20score%20increased.%C2%A0The%20result" target="_blank" rel="noopener noreferrer" data-placement="top" data-title="Linkedin" data-toggle="tooltip" title="" data-original-title="Linkedin"><span class="screen-reader-text">Linkedin</span></a><a class="fusion-social-network-icon fusion-tooltip fusion-reddit fusion-icon-reddit" style="color:#84aead;" href="http://reddit.com/submit?url=https://www.nervanasys.com/demystifying-deep-reinforcement-learning/&amp;title=Guest%20Post%20%28Part%20I%29%3A%20Demystifying%20Deep%20Reinforcement%20Learning" target="_blank" rel="noopener noreferrer" data-placement="top" data-title="Reddit" data-toggle="tooltip" title="" data-original-title="Reddit"><span class="screen-reader-text">Reddit</span></a><a class="fusion-social-network-icon fusion-tooltip fusion-tumblr fusion-icon-tumblr" style="color:#84aead;" href="http://www.tumblr.com/share/link?url=https%3A%2F%2Fwww.nervanasys.com%2Fdemystifying-deep-reinforcement-learning%2F&amp;name=Guest%20Post%20%28Part%20I%29%3A%20Demystifying%20Deep%20Reinforcement%20Learning&amp;description=Two%20years%20ago%2C%20a%20small%20company%20in%20London%20called%20DeepMind%20uploaded%20their%20pioneering%20paper%20%E2%80%9CPlaying%20Atari%20with%20Deep%20Reinforcement%20Learning%E2%80%9D%20to%20Arxiv.%20In%20this%20paper%20they%20demonstrated%20how%20a%20computer%20learned%20to%20play%20Atari%202600%20video%20games%20by%20observing%20just%20the%20screen%20pixels%20and%20receiving%20a%20reward%20when%20the%20game%20score%20increased.%C2%A0The%20result" target="_blank" rel="noopener noreferrer" data-placement="top" data-title="Tumblr" data-toggle="tooltip" title="" data-original-title="Tumblr"><span class="screen-reader-text">Tumblr</span></a><a class="fusion-social-network-icon fusion-tooltip fusion-googleplus fusion-icon-googleplus" style="color:#84aead;" href="https://plus.google.com/share?url=https://www.nervanasys.com/demystifying-deep-reinforcement-learning/" onclick="javascript:window.open(this.href,&#39;&#39;, &#39;menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600&#39;);return false;" target="_blank" rel="noopener noreferrer" data-placement="top" data-title="Google+" data-toggle="tooltip" title="" data-original-title="Google+"><span class="screen-reader-text">Google+</span></a><a class="fusion-social-network-icon fusion-tooltip fusion-pinterest fusion-icon-pinterest" style="color:#84aead;" href="http://pinterest.com/pin/create/button/?url=https%3A%2F%2Fwww.nervanasys.com%2Fdemystifying-deep-reinforcement-learning%2F&amp;description=Two%20years%20ago%2C%20a%20small%20company%20in%20London%20called%20DeepMind%20uploaded%20their%20pioneering%20paper%20%E2%80%9CPlaying%20Atari%20with%20Deep%20Reinforcement%20Learning%E2%80%9D%20to%20Arxiv.%20In%20this%20paper%20they%20demonstrated%20how%20a%20computer%20learned%20to%20play%20Atari%202600%20video%20games%20by%20observing%20just%20the%20screen%20pixels%20and%20receiving%20a%20reward%20when%20the%20game%20score%20increased.%C2%A0The%20result&amp;media=https%3A%2F%2Fwww.nervanasys.com%2Fwp-content%2Fuploads%2F2015%2F12%2FScreen-Shot-2015-12-21-at-12.01.15-PM.png" target="_blank" rel="noopener noreferrer" data-placement="top" data-title="Pinterest" data-toggle="tooltip" title="" data-original-title="Pinterest"><span class="screen-reader-text">Pinterest</span></a><a class="fusion-social-network-icon fusion-tooltip fusion-vk fusion-icon-vk" style="color:#84aead;" href="http://vkontakte.ru/share.php?url=https%3A%2F%2Fwww.nervanasys.com%2Fdemystifying-deep-reinforcement-learning%2F&amp;title=Guest%20Post%20%28Part%20I%29%3A%20Demystifying%20Deep%20Reinforcement%20Learning&amp;description=Two%20years%20ago%2C%20a%20small%20company%20in%20London%20called%20DeepMind%20uploaded%20their%20pioneering%20paper%20%E2%80%9CPlaying%20Atari%20with%20Deep%20Reinforcement%20Learning%E2%80%9D%20to%20Arxiv.%20In%20this%20paper%20they%20demonstrated%20how%20a%20computer%20learned%20to%20play%20Atari%202600%20video%20games%20by%20observing%20just%20the%20screen%20pixels%20and%20receiving%20a%20reward%20when%20the%20game%20score%20increased.%C2%A0The%20result" target="_blank" rel="noopener noreferrer" data-placement="top" data-title="Vk" data-toggle="tooltip" title="" data-original-title="Vk"><span class="screen-reader-text">Vk</span></a><a class="fusion-social-network-icon fusion-tooltip fusion-mail fusion-icon-mail fusion-last-social-icon" style="color:#84aead;" href="mailto:?subject=Guest%20Post%20(Part%20I):%20Demystifying%20Deep%20Reinforcement%20Learning&amp;body=https://www.nervanasys.com/demystifying-deep-reinforcement-learning/" target="_self" rel="noopener noreferrer" data-placement="top" data-title="Email" data-toggle="tooltip" title="" data-original-title="Email"><span class="screen-reader-text">Email</span></a><div class="fusion-clearfix"></div></div></div>			</div>
												<div class="about-author">
												<div class="fusion-title fusion-title-size-three sep-double" style="margin-top:0px;margin-bottom:31px;"><h3 class="title-heading-left" data-fontsize="15" data-lineheight="22">About the Author: 						<a href="https://www.nervanasys.com/author/tambet/" title="Posts by Tambet Matiisen" rel="author">Tambet Matiisen</a>						</h3><div class="title-sep-container"><div class="title-sep sep-double"></div></div></div>						<div class="about-author-container">
							<div class="avatar">
								<img alt="" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/64ff9030ad3b0b7b46dbb4d2881af55f" srcset="https://secure.gravatar.com/avatar/64ff9030ad3b0b7b46dbb4d2881af55f?s=144&amp;d=mm&amp;r=g 2x" class="avatar avatar-72 photo" height="72" width="72">							</div>
							<div class="description">
								Tambet Matiisen is a PhD student in University of Tartu, Estonia. After working in industry for a while and founding his own SaaS startup, he decided to join academia again. He hates programming and is interested in making the machines learn the same way as humans do. He shares his life with dog-obsessed wife and two out of hand kids. At less busy moments he enjoys obscure flashbacks from 90s, like old-skool breakbeat or MSX home computers.							</div>
						</div>
					</div>
								
									</article>
		</div>
<div id="sidebar" class="sidebar fusion-widget-area fusion-content-widget-area" style="float: right;">
						
			<div id="search-2" class="widget widget_search"><form role="search" class="searchform" method="get" action="https://www.nervanasys.com/">
	<div class="search-table">
		<div class="search-field">
			<input type="text" value="" name="s" class="s" placeholder="Search ...">
		</div>
		<div class="search-button">
			<input type="submit" class="searchsubmit" value="">
		</div>
	</div>
</form>
</div><div id="text-27" class="widget widget_text">			<div class="textwidget">


<!-- Form created by Optin Forms plugin by Codeleon: create beautiful optin forms with ease! -->
<!-- http://codeleon.com/products/optin-forms/ -->
<div id="optinforms-form5-container"><form method="post" action="https://nervanasys.us8.list-manage.com/subscribe/post?u=1512ed86c2200d8d99a12512f&amp;id=126f5ecfd5"><div id="optinforms-form5" style="background:#ffffff;"><div id="optinforms-form5-container-left"><div id="optinforms-form5-title" style="font-family:News Cycle; font-size:24px; line-height:24px; color:#fb6a13">JOIN OUR NEWSLETTER</div><!--optinforms-form5-title--><input type="text" id="optinforms-form5-name-field" name="FNAME" placeholder="enter your name" style="font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000000"><input type="text" id="optinforms-form5-email-field" name="EMAIL" placeholder="enter your e-mail" style="font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000000"><input type="submit" name="submit" id="optinforms-form5-button" value="SUBSCRIBE" style="font-family:Trebuchet MS, Arial, sans-serif; font-size:16px; color:#FFFFFF; background-color:#cde0df"></div><!--optinforms-form5-container-left--><div id="optinforms-form5-container-right"><div id="optinforms-form5-subtitle" style="font-family:Georgia; font-size:16px; color:#cccccc">Join over 3.000 visitors who are receiving our newsletter and learn how to optimize your blog for search engines, find free traffic, and monetize your website.</div><!--optinforms-form5-subtitle--><div id="optinforms-form5-disclaimer" style="font-family:Georgia, Times New Roman, Times, serif; font-size:14px; color:#727272">We hate spam. Your email address will not be sold or shared with anyone else.</div><!--optinforms-form5-disclaimer--></div><!--optinforms-form5-container-right--><div class="clear"></div></div><!--optinforms-form5--><div class="clear"></div></form></div><!--optinforms-form5-container--><div class="clear"></div>
<!-- / Optin Forms -->

<style type="text/css">#optinforms-form5-title{display:none;}#optinforms-form5-subtitle{display:none;}#optinforms-form5-disclaimer{margin:0 20px;}#optinforms-form5-disclaimer{display:none;}#optinforms-form5-container-right{display:none;}#optinforms-form5-container-left{margin:10px 0;width:100%;}</style></div>
		</div><div id="categories-2" class="widget widget_categories"><div class="heading"><h4 class="widget-title" data-fontsize="13" data-lineheight="14">Categories</h4></div>		<ul>
	<li class="cat-item cat-item-53"><a href="https://www.nervanasys.com/category/developer/">Developer</a>
</li>
	<li class="cat-item cat-item-50"><a href="https://www.nervanasys.com/category/general/">General</a>
</li>
	<li class="cat-item cat-item-52"><a href="https://www.nervanasys.com/category/impact/">Impact</a>
</li>
	<li class="cat-item cat-item-51"><a href="https://www.nervanasys.com/category/life-at-nervana/">Life at Nervana</a>
</li>
	<li class="cat-item cat-item-54"><a href="https://www.nervanasys.com/category/product/">Product</a>
</li>
		</ul>
</div>	</div>

				</div>  <!-- fusion-row -->
			</div>  <!-- #main -->
			
			
			
			
										
				<div class="fusion-footer">

																
						<footer class="fusion-footer-widget-area fusion-widget-area">
							<div class="fusion-row">
								<div class="fusion-columns fusion-columns-4 fusion-widget-area">
									
																																							<div class="fusion-column col-lg-3 col-md-3 col-sm-3">
												<div id="text-16" class="fusion-footer-widget-column widget widget_text">			<div class="textwidget"><p><img src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/nervanasystemeslogoheader1.png" width="175px"><br>
<br><br></p>
<div class="alignleft">
<div class="fusion-social-links">
<div class="fusion-social-networks">
<div class="fusion-social-networks-wrapper"><a class="fusion-social-network-icon fusion-tooltip fusion-facebook fusion-icon-facebook" style="color:#ffffff;" target="_blank" rel="noopener noreferrer" href="http://www.facebook.com/nervanasys" title="Facebook"></a><a class="fusion-social-network-icon fusion-tooltip fusion-twitter fusion-icon-twitter" style="color:#ffffff;" target="_blank" rel="noopener noreferrer" href="http://www.twitter.com/nervanasys" title="Twitter"></a><a class="fusion-social-network-icon fusion-tooltip fusion-youtube fusion-icon-youtube" style="color:#ffffff;" target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/channel/UChQAxUd_onSPjBejB_Awk7Q" title="Youtube"></a><a class="fusion-social-network-icon fusion-tooltip fusion-linkedin fusion-icon-linkedin" style="color:#ffffff;" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/company/3628074?trk=prof-exp-company-name" title="Linkedin"></a><a class="fusion-social-network-icon fusion-tooltip fusion-googleplus fusion-icon-googleplus" style="color:#ffffff;" target="_blank" rel="noopener noreferrer" href="https://plus.google.com/u/0/b/109836765639977525859/109836765639977525859/" title="Googleplus"></a><a class="fusion-social-network-icon fusion-tooltip fusion-mail fusion-icon-mail" style="color:#ffffff;" target="_self" rel="noopener noreferrer" href="mailto:info@nervanasys.com" title="Mail"></a></div>
</div>
</div>
</div>
</div>
		<div style="clear:both;"></div></div>																																				</div>
																																								<div class="fusion-column col-lg-3 col-md-3 col-sm-3">
												<div id="text-3" class="fusion-footer-widget-column widget widget_text">			<div class="textwidget"><p style="font-size: 13px;"><a href="https://www.nervanasys.com/products/"><span style="color: #ffffff;"><strong>PRODUCTS</strong></span></a><strong><br><br></strong><strong><a href="https://www.nervanasys.com/technology/"><span style="color: #ffffff;">TECHNOLOGY</span></a></strong><br>
<a href="https://www.nervanasys.com/technology/neon/"><span style="color: #ffffff;">neon</span></a><br>
<a href="https://www.nervanasys.com/technology/engine/"><span style="color: #ffffff;">Nervana Engine</span></a></p>
</div>
		<div style="clear:both;"></div></div>																																				</div>
																																								<div class="fusion-column col-lg-3 col-md-3 col-sm-3">
												<div id="text-29" class="fusion-footer-widget-column widget widget_text">			<div class="textwidget"><p style="font-size: 13px;"><strong><a href="https://www.nervanasys.com/solutions/"><span style="color: #ffffff;">SOLUTIONS</span></a></strong><br>
<a href="https://www.nervanasys.com/solutions/healthcare/"><span style="color: #ffffff;">Healthcare</span></a><br>
<a href="https://www.nervanasys.com/solutions/agriculture/"><span style="color: #ffffff;">Agriculture</span></a><br>
<a href="https://www.nervanasys.com/solutions/finance/"><span style="color: #ffffff;">Finance</span></a><br>
<a href="https://www.nervanasys.com/solutions/onlineservices/"><span style="color: #ffffff;">Online Services</span></a><br>
<a href="https://www.nervanasys.com/solutions/automotive/"><span style="color: #ffffff;">Automotive</span></a><br>
<a href="https://www.nervanasys.com/solutions/energy/"><span style="color: #ffffff;">Energy</span></a></p>
</div>
		<div style="clear:both;"></div></div>																																				</div>
																																								<div class="fusion-column fusion-column-last col-lg-3 col-md-3 col-sm-3">
												<div id="text-30" class="fusion-footer-widget-column widget widget_text">			<div class="textwidget"><p style="font-size: 13px;"><strong><a href="https://www.nervanasys.com/about/"><span style="color: #ffffff;">ABOUT</span></a></strong><br>
<a href="https://www.nervanasys.com/press/"><span style="color: #ffffff;">Press</span></a><br>
<a href="https://www.nervanasys.com/blog/"><span style="color: #ffffff;">Blog</span></a><br>
<a href="https://www.nervanasys.com/team/"><span style="color: #ffffff;">Team</span></a><br>
<a href="https://www.nervanasys.com/investors-and-advisors/"><span style="color: #ffffff;">Investors and Advisors</span></a><br>
<a href="https://www.nervanasys.com/careers/"><span style="color: #ffffff;">Careers</span></a><br>
<a href="https://www.nervanasys.com/culture/"><span style="color: #ffffff;">Culture</span></a><br>
<a href="https://www.nervanasys.com/contact/"><span style="color: #ffffff;">Contact</span></a></p>
</div>
		<div style="clear:both;"></div></div>																																				</div>
																																																									
									<div class="fusion-clearfix"></div>
								</div> <!-- fusion-columns -->
							</div> <!-- fusion-row -->
						</footer> <!-- fusion-footer-widget-area -->
					
														</div> <!-- fusion-footer -->
					</div> <!-- wrapper -->

				
		<a class="fusion-one-page-text-link fusion-page-load-link"></a>

		<!-- W3TC-include-js-head -->

		<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/jquery.form.min.js"></script>
<script type="text/javascript">
/* <![CDATA[ */
var _wpcf7 = {"loaderUrl":"https:\/\/www.nervanasys.com\/wp-content\/plugins\/contact-form-7\/images\/ajax-loader.gif","recaptcha":{"messages":{"empty":"Please verify that you are not a robot."}},"sending":"Sending ...","cached":"1"};
/* ]]> */
</script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/scripts.js"></script>
<script type="text/javascript">
/* <![CDATA[ */
var toTopscreenReaderText = {"label":"Go to Top"};
var avadaVars = {"admin_ajax":"https:\/\/www.nervanasys.com\/wp-admin\/admin-ajax.php","admin_ajax_nonce":"f7c7238dd7","protocol":"1","theme_url":"https:\/\/www.nervanasys.com\/wp-content\/themes\/Avada","dropdown_goto":"Go to...","mobile_nav_cart":"Shopping Cart","page_smoothHeight":"false","flex_smoothHeight":"false","language_flag":"","infinite_blog_finished_msg":"<em>All posts displayed.<\/em>","infinite_finished_msg":"<em>All items displayed.<\/em>","infinite_blog_text":"<em>Loading the next set of posts...<\/em>","portfolio_loading_text":"<em>Loading Portfolio Items...<\/em>","faqs_loading_text":"<em>Loading FAQ Items...<\/em>","order_actions":"Details","avada_rev_styles":"1","avada_styles_dropdowns":"1","blog_grid_column_spacing":"40","blog_pagination_type":"Pagination","carousel_speed":"2500","counter_box_speed":"1000","content_break_point":"800","disable_mobile_animate_css":"0","disable_mobile_image_hovers":"1","portfolio_pagination_type":"Pagination","form_bg_color":"#ffffff","header_transparency":"0","header_padding_bottom":"0px","header_padding_top":"0px","header_position":"Top","header_sticky":"1","header_sticky_tablet":"1","header_sticky_mobile":"1","header_sticky_type2_layout":"menu_only","sticky_header_shrinkage":"1","is_responsive":"1","is_ssl":"true","isotope_type":"masonry","layout_mode":"wide","lightbox_animation_speed":"Fast","lightbox_arrows":"1","lightbox_autoplay":"0","lightbox_behavior":"all","lightbox_desc":"1","lightbox_deeplinking":"1","lightbox_gallery":"1","lightbox_opacity":"0.8","lightbox_path":"vertical","lightbox_post_images":"1","lightbox_skin":"light","lightbox_slideshow_speed":"5000","lightbox_social":"1","lightbox_title":"1","lightbox_video_height":"720","lightbox_video_width":"1280","logo_alignment":"Left","logo_margin_bottom":"31px","logo_margin_top":"31px","megamenu_max_width":"1100","mobile_menu_design":"modern","nav_height":"83","nav_highlight_border":"3","page_title_fading":"0","pagination_video_slide":"0","related_posts_speed":"2500","submenu_slideout":"1","side_header_break_point":"1023","sidenav_behavior":"Hover","site_width":"1100px","slider_position":"below","slideshow_autoplay":"1","slideshow_speed":"7000","smooth_scrolling":"0","status_lightbox":"1","status_totop_mobile":"1","status_vimeo":"1","status_yt":"1","testimonials_speed":"8000","tfes_animation":"sides","tfes_autoplay":"1","tfes_interval":"3000","tfes_speed":"800","tfes_width":"150","title_style_type":"double","title_margin_top":"0px","title_margin_bottom":"31px","typography_responsive":"1","typography_sensitivity":"0.6","typography_factor":"1.5","woocommerce_shop_page_columns":"","woocommerce_checkout_error":"Not all fields have been filled in correctly.","side_header_width":"0"};
/* ]]> */
</script>
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/main.min.js" async=""></script> 
<!--[if IE 9]>
<script type='text/javascript' src='https://www.nervanasys.com/wp-content/themes/Avada/assets/js/avada-ie9.js?ver=5.0.2'></script>
<![endif]-->
<!--[if lt IE 9]>
<script type='text/javascript' src='https://www.nervanasys.com/wp-content/themes/Avada/assets/js/respond.js?ver=5.0.2'></script>
<![endif]-->
<script type="text/javascript" src="./Guest Post (Part I)_ Demystifying Deep Reinforcement Learning - Nervana_files/wp-embed.min.js"></script>

			


<!-- Dynamic page generated in 0.409 seconds. -->
<!-- Cached page generated by WP-Super-Cache on 2016-11-06 14:51:50 -->

<!-- super cache --><div class="to-top-container"><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/#" id="toTop" style="display: inline; opacity: 0.87296;"><span id="toTopHover"></span><span class="screen-reader-text">Go to Top</span></a></div></body></html>